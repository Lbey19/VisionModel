# ğŸ¤– VisionModel - Assistant IA de Survie & PrÃ©paration

## ğŸ“ Description
**VisionModel** est un systÃ¨me d'intelligence artificielle conversationnelle spÃ©cialisÃ© dans la **survie**, la **prÃ©paration d'urgence** et la **gestion de crise communautaire**. BasÃ© sur la technologie RAG (Retrieval-Augmented Generation), il combine une base de connaissances documentaires Ã©tendue avec des modÃ¨les IA locaux Ollama pour fournir des rÃ©ponses prÃ©cises et contextuelles.

## âœ¨ FonctionnalitÃ©s Principales

### ğŸ¯ SystÃ¨me RAG AvancÃ©
- **Recherche sÃ©mantique** intelligente dans 15+ documents spÃ©cialisÃ©s
- **Classification automatique** : mode "survie" vs mode "gÃ©nÃ©ral"
- **Sources citÃ©es** : traÃ§abilitÃ© complÃ¨te des informations
- **Index vectoriel** optimisÃ© avec nomic-embed-text

### ğŸ¤– IA Locale Puissante
- **Gemma2:2b** : ModÃ¨le principal, rÃ©ponses rapides et prÃ©cises
- **Llama3.1:8b** : ModÃ¨le alternatif pour analyses complexes
- **Nomic-embed-text** : Embeddings haute qualitÃ© pour la recherche
- **Fonctionnement hors-ligne** : aucune dÃ©pendance cloud

### ğŸ’¬ Interface Utilisateur
- **Chat web interactif** : interface moderne et responsive
- **API REST complÃ¨te** : intÃ©gration facile dans d'autres applications
- **Format structurÃ©** : rÃ©ponses organisÃ©es avec actions immÃ©diates et plans
- **Affichage des sources** : transparence sur l'origine des informations

### ğŸ“š Base de Connaissances Ã‰tendue (15 Documents)
- **Alimentation** : conservation, stockage, rotation des vivres
- **Eau potable** : purification, stockage, techniques d'urgence
- **Premiers secours** : soins de base, procÃ©dures d'urgence
- **Communications** : PMR446, protocoles radio, signalisation
- **Ã‰nergie** : solutions alternatives, Ã©clairage, batteries
- **Abris** : protection thermique, isolation, ventilation
- **HygiÃ¨ne** : assainissement, dÃ©chets, prÃ©vention maladies  
- **SÃ©curitÃ©** : surveillance pÃ©rimÃ¨tre, procÃ©dures d'alerte
- **Organisation** : gestion de groupe, rÃ©solution conflits
- **Psychologie** : gestion stress, soutien moral, cohÃ©sion
- **RÃ©parations** : maintenance, rÃ©cupÃ©ration, outils
- **Navigation** : orientation, signalisation secours
- **Ã‰vacuation** : procÃ©dures d'urgence, codes d'alerte

## ğŸ› ï¸ Stack Technique Moderne
- **Backend** : Node.js 18+ + Express.js (architecture modulaire)
- **IA Locale** : Ollama (gemma2:2b, llama3.1:8b, nomic-embed-text)
- **RAG System** : Recherche vectorielle cosinus + cache intelligent
- **Frontend** : HTML5/CSS3/ES6+ vanilla (0 dÃ©pendance)
- **Base donnÃ©es** : JSON vectoriel optimisÃ©
- **Plateforme** : Windows/Linux/macOS (scripts d'installation inclus)

## ğŸš€ Installation & Configuration

### PrÃ©requis SystÃ¨me
- **Node.js** v18+ ([tÃ©lÃ©charger](https://nodejs.org/))
- **Ollama** ([tÃ©lÃ©charger](https://ollama.ai/download))
- **Git** ([tÃ©lÃ©charger](https://git-scm.com/downloads))
- **Espace disque** : ~8GB pour les modÃ¨les IA

### ğŸ”§ Installation Automatique (Windows)
```powershell
# Cloner le projet
git clone https://github.com/Lbey19/VisionModel.git
cd VisionModel

# Installation complÃ¨te automatique
.\install-new-pc.bat
```

### ğŸ§ Installation Manuelle (Tous OS)
```bash
# 1. Cloner le projet
git clone https://github.com/Lbey19/VisionModel.git
cd VisionModel

# 2. Installer les dÃ©pendances Node.js
npm install

# 3. Configurer l'environnement (Windows uniquement)
.\setup-environment.ps1

# 4. TÃ©lÃ©charger les modÃ¨les IA (patient, ~6GB)
ollama pull gemma2:2b
ollama pull llama3.1:8b
ollama pull nomic-embed-text

# 5. Construire l'index RAG
npm run ingest

# 6. DÃ©marrer le serveur
npm start
# OU sur Windows :
.\start-server.bat
```

### ğŸŒ Points d'AccÃ¨s
- **Interface Chat** : http://localhost:3001
- **API SantÃ©** : GET http://localhost:3001/api/health  
- **API Chat** : POST http://localhost:3001/api/chat
- **Fichiers statiques** : http://localhost:3001/public/

## ğŸ“– Guide d'Utilisation

### ğŸ–¥ï¸ Interface Web
1. **AccÃ¨s** : Ouvrir http://localhost:3001
2. **Chat** : Poser vos questions dans la zone de saisie
3. **RÃ©ponses structurÃ©es** : Format survie avec actions immÃ©diates et sources
4. **Sources visibles** : Cliquer sur les sources pour voir les documents utilisÃ©s

### ğŸ¯ Exemples de Questions par CatÃ©gorie

**ğŸ’§ Eau & Alimentation**
- "Comment purifier de l'eau trouble avec de la javel ?"
- "Combien de gouttes de javel par litre d'eau ?"  
- "Techniques de conservation alimentaire sans frigo ?"

**ğŸ  Abri & Ã‰nergie**
- "Comment chauffer une piÃ¨ce sans Ã©lectricitÃ© ?"
- "Solutions d'Ã©clairage d'urgence avec batteries ?"
- "Isolation thermique avec matÃ©riaux de rÃ©cupÃ©ration ?"

**ğŸ“¡ Communication & SÃ©curitÃ©**
- "Quels canaux PMR446 recommandÃ©s pour l'urgence ?"
- "Comment organiser la surveillance d'un pÃ©rimÃ¨tre ?"
- "ProcÃ©dures d'Ã©vacuation et codes d'alerte ?"

**ğŸ©¹ SantÃ© & HygiÃ¨ne**
- "Comment traiter une brÃ»lure en urgence ?"
- "DÃ©sinfection d'urgence sans produits chimiques ?"
- "Gestion du stress en situation de crise ?"

### ğŸ”Œ API REST ComplÃ¨te

#### VÃ©rification du Statut
```bash
curl http://localhost:3001/api/health
```

#### Chat avec l'IA
```javascript
// JavaScript/Node.js
const response = await fetch('http://localhost:3001/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ 
    message: "Comment organiser une Ã©quipe de survie ?" 
  })
});

const data = await response.json();
console.log('RÃ©ponse:', data.reply);
console.log('Sources:', data.sources);
console.log('Mode:', data.mode); // "survival" ou "general"
```

```bash
# PowerShell
Invoke-RestMethod -Uri "http://localhost:3001/api/chat" `
  -Method POST `
  -ContentType "application/json" `
  -Body '{"message":"Comment stocker de l eau potable?"}'
```

### ğŸ“Š RÃ©ponse API Format
```json
{
  "reply": "# Ã‰valuation rapide\n...",
  "sources": ["eau_potable.md", "hygiene_assainissement.md"],
  "modelUsed": "gemma2:2b",
  "mode": "survival"
}
```

## ğŸ“ Architecture du Projet

```
VisionModel/
â”œâ”€â”€ ğŸŒ Backend & API
â”‚   â”œâ”€â”€ server.js                    # Serveur Express principal
â”‚   â”œâ”€â”€ api.js                       # Routes API (/health, /chat)
â”‚   â”œâ”€â”€ rag.js                       # Moteur de recherche RAG
â”‚   â””â”€â”€ ingest.js                    # Construction index vectoriel
â”‚
â”œâ”€â”€ ğŸ“Š Base de DonnÃ©es  
â”‚   â”œâ”€â”€ rag.index.json               # Index vectoriel (15 docs)
â”‚   â””â”€â”€ context/community.json       # Profil communautÃ©
â”‚
â”œâ”€â”€ ğŸ¨ Interface Utilisateur
â”‚   â””â”€â”€ public/
â”‚       â”œâ”€â”€ index.html               # Interface chat
â”‚       â”œâ”€â”€ script.js                # Logique frontend
â”‚       â””â”€â”€ styles.css               # Styles modernes
â”‚
â”œâ”€â”€ ğŸ“š Base de Connaissances (15 docs)
â”‚   â””â”€â”€ docs/
â”‚       â”œâ”€â”€ ğŸ’§ eau_potable.md
â”‚       â”œâ”€â”€ ğŸ©¹ premiers_secours.txt
â”‚       â”œâ”€â”€ ğŸ“¡ communications_locales.md
â”‚       â”œâ”€â”€ ğŸ  abris_froid_chaleur.md
â”‚       â”œâ”€â”€ âš¡ energie_lumiere.md
â”‚       â”œâ”€â”€ ğŸ”’ securite_discretion.md
â”‚       â”œâ”€â”€ ğŸ½ï¸ alimentation_conservation.md
â”‚       â”œâ”€â”€ ğŸ”¥ chauffage_ventilation.md
â”‚       â”œâ”€â”€ ğŸ§¼ hygiene_assainissement.md
â”‚       â”œâ”€â”€ ğŸ§­ navigation_orientation.md
â”‚       â”œâ”€â”€ ğŸ‘¥ organisation_communautaire.md
â”‚       â”œâ”€â”€ ğŸ§  psychologie_gestion_stress.md
â”‚       â”œâ”€â”€ ğŸ”§ reparations_maintenance.md
â”‚       â”œâ”€â”€ ğŸ›¡ï¸ securite_perimetres.md
â”‚       â””â”€â”€ ğŸš¨ signalisation_evacuation.md
â”‚
â”œâ”€â”€ ğŸ› ï¸ Scripts & Outils
â”‚   â”œâ”€â”€ start-server.bat            # DÃ©marrage Windows
â”‚   â”œâ”€â”€ setup-environment.ps1       # Config PATH Ollama
â”‚   â”œâ”€â”€ install-new-pc.bat          # Installation complÃ¨te
â”‚   â””â”€â”€ create-package.bat          # Package portable
â”‚
â”œâ”€â”€ ğŸ“ Configuration
â”‚   â”œâ”€â”€ package.json                # DÃ©pendances npm
â”‚   â”œâ”€â”€ .gitignore                  # Exclusions Git
â”‚   â””â”€â”€ DEPLOYMENT.md               # Guide dÃ©ploiement
â”‚
â””â”€â”€ ğŸ¤– ModÃ¨les IA (local)
    â””â”€â”€ ollama-windows-amd64.exe    # ExÃ©cutable Ollama
```

## âš™ï¸ Configuration AvancÃ©e

### Variables d'Environnement
```bash
# Configuration serveur
PORT=3001                           # Port d'Ã©coute
OLLAMA_URL=http://127.0.0.1:11434   # URL Ollama local

# ModÃ¨les IA utilisÃ©s
LLM_MODEL=gemma2:2b                 # ModÃ¨le principal (rÃ©ponses)
EMBED_MODEL=nomic-embed-text        # ModÃ¨le embeddings (recherche)
```

### ğŸ¤– ModÃ¨les IA SupportÃ©s

| ModÃ¨le | Taille | Usage | Performance | RAM |
|--------|--------|-------|-------------|-----|
| **gemma2:2b** | ~1.6GB | Principal (dÃ©faut) | Rapide, prÃ©cis | 4GB+ |
| **llama3.1:8b** | ~4.7GB | Alternatif prÃ©cis | Lent, trÃ¨s prÃ©cis | 8GB+ |
| **nomic-embed-text** | ~274MB | Embeddings RAG | Recherche | 2GB+ |

### ğŸ›ï¸ Modes de Fonctionnement

**Mode Survie** (dÃ©tection automatique)
- Mots-clÃ©s : survie, eau, javel, PMR446, premiers secours, etc.
- Format structurÃ© : Ã‰valuation â†’ Actions â†’ Plan â†’ MatÃ©riel â†’ Vigilance
- Sources RAG prioritaires

**Mode GÃ©nÃ©ral**  
- Questions encyclopÃ©diques, culture, dÃ©finitions
- Format libre, pas de structure survie
- Connaissances internes du modÃ¨le

## ï¿½ DÃ©veloppement & Extension

### ğŸ“„ Ajouter des Documents
```bash
# 1. CrÃ©er le document (formats supportÃ©s)
echo "# Mon Guide" > docs/nouveau_guide.md     # Markdown
echo "Contenu texte" > docs/guide.txt          # Texte brut
# Formats : .md, .txt, .docx (PDF via conversion)

# 2. Reconstruire l'index RAG
npm run ingest
# OU
node ingest.js

# 3. RedÃ©marrer le serveur
npm start
```

### ğŸ”„ Changer le ModÃ¨le IA
```bash
# Option 1 : Variable d'environnement
set LLM_MODEL=llama3.1:8b
npm start

# Option 2 : Modifier start-server.bat
# set LLM_MODEL=llama3.1:8b

# Option 3 : Modifier api.js
# const MODEL = process.env.LLM_MODEL || "llama3.1:8b";
```

### ğŸ“Š Scripts NPM Disponibles
```bash
npm start        # DÃ©marrer le serveur
npm run ingest   # Reconstruire l'index RAG
```

## ğŸ“¦ DÃ©ploiement & Distribution

### ğŸ“€ Package Portable Windows
```batch
# CrÃ©er un package ZIP autonome
.\create-package.bat
# â†’ GÃ©nÃ¨re VisionModel-Portable.zip
```

### ğŸ–¥ï¸ Installation sur Nouveau PC
```batch
# Installation automatique complÃ¨te
.\install-new-pc.bat
# Installe : Node.js + Ollama + DÃ©pendances + ModÃ¨les
```

### ğŸ³ Docker (BientÃ´t)
```bash
# Build de l'image (en dÃ©veloppement)
docker build -t visionmodel .
docker run -p 3001:3001 visionmodel
```

### â˜ï¸ DÃ©ploiement Cloud
- **Heroku** : Voir `DEPLOYMENT.md`
- **VPS/Serveur** : Node.js + reverse proxy nginx
- **âš ï¸ Limitation** : NÃ©cessite Ollama installÃ© sur le serveur

## ğŸ› ï¸ Maintenance & DÃ©pannage

### ğŸ©º Diagnostic Rapide
```bash
# VÃ©rifier Ollama
ollama list

# VÃ©rifier Node.js
node --version
npm --version

# Tester l'API
curl http://localhost:3001/api/health
```

### ğŸš¨ ProblÃ¨mes Courants

| ProblÃ¨me | Cause | Solution |
|----------|-------|----------|
| "Ollama not found" | PATH incorrect | `.\setup-environment.ps1` |
| "Port 3001 in use" | Serveur dÃ©jÃ  lancÃ© | `Get-Process node \| Stop-Process` |
| "No response from model" | ModÃ¨le non tÃ©lÃ©chargÃ© | `ollama pull gemma2:2b` |
| "RAG index empty" | Documents non indexÃ©s | `npm run ingest` |

## ğŸ“Š Statistiques du Projet

- **ğŸ“š Base de connaissances** : 15 documents spÃ©cialisÃ©s
- **ğŸ” Index RAG** : ~15 chunks vectoriels
- **ğŸ¤– ModÃ¨les IA** : 3 modÃ¨les (6,5GB total)
- **ğŸ’» Code source** : ~800 lignes (JS/HTML/CSS)
- **ğŸ“ Documentation** : 5 fichiers guides
- **ğŸ› ï¸ Scripts** : 6 outils d'installation/maintenance

## ğŸ¤ Contribution & CommunautÃ©

### ğŸŒŸ Contribuer au Projet
1. **Fork** le repository
2. **Clone** votre fork : `git clone https://github.com/VotreUsername/VisionModel.git`
3. **CrÃ©er** une branche : `git checkout -b feature/nouvelle-fonctionnalitÃ©`
4. **DÃ©velopper** et tester vos modifications
5. **Commit** : `git commit -m "âœ¨ Ajout nouvelle fonctionnalitÃ©"`
6. **Push** : `git push origin feature/nouvelle-fonctionnalitÃ©`
7. **Pull Request** vers la branche `main`

### ğŸ“‹ Types de Contributions RecherchÃ©es
- **ğŸ“š Nouveaux documents** de survie/prÃ©paration
- **ğŸŒ Traductions** (anglais, espagnol, etc.)
- **ğŸ¨ AmÃ©liorations UI/UX** de l'interface
- **âš¡ Optimisations** performance RAG
- **ğŸ› Corrections** de bugs
- **ğŸ“– Documentation** supplÃ©mentaire

### ğŸ’¬ CommunautÃ©
- **ğŸ› Issues** : [GitHub Issues](https://github.com/Lbey19/VisionModel/issues)
- **ğŸ’¡ Discussions** : [GitHub Discussions](https://github.com/Lbey19/VisionModel/discussions) 
- **ğŸ“§ Contact** : Via GitHub ou Issues

## ğŸ“„ Licence & LÃ©gal

**Licence MIT** - Utilisation libre pour projets personnels et commerciaux

### âš ï¸ Avertissements Importants
- **ğŸ©º MÃ©dical** : Informations Ã  titre Ã©ducatif uniquement, consulter un professionnel
- **âš–ï¸ LÃ©gal** : Respecter les lois locales (radio, sÃ©curitÃ©, etc.)
- **ğŸ›¡ï¸ SÃ©curitÃ©** : Tester les techniques en environnement sÃ»r
- **ğŸ¤– IA** : Les rÃ©ponses peuvent contenir des erreurs, toujours vÃ©rifier

## ğŸ†˜ Support & Documentation

- **ï¿½ Guide complet** : `DEPLOYMENT.md`
- **ğŸ› ï¸ Installation** : Scripts automatiques inclus
- **ğŸ› ProblÃ¨mes** : GitHub Issues avec logs dÃ©taillÃ©s
- **ğŸ’¬ Questions** : GitHub Discussions pour l'aide communautaire

---

## ğŸ¯ Roadmap Futur

- [ ] ï¿½ **Containerisation Docker**
- [ ] ğŸŒ **Interface multilingue** 
- [ ] ğŸ“± **Application mobile** (PWA)
- [ ] ğŸ”Š **SynthÃ¨se vocale** des rÃ©ponses
- [ ] ğŸ“¸ **Analyse d'images** (blessures, plantes, etc.)
- [ ] ğŸ—ºï¸ **Cartes hors-ligne** intÃ©grÃ©es
- [ ] ğŸ“¡ **Mode dÃ©connectÃ©** complet

---

**DÃ©veloppÃ© avec â¤ï¸ pour la communautÃ© de prÃ©paration et survie**  
*Un projet open-source pour l'autonomie et la rÃ©silience communautaire*