# 🛡️ VisionModel - L'IA Gardienne de l'Humanité

> *"Quand Ultron a éteint les lumières du monde, Vision est resté allumé."*

## 🌟 La Mission
Dans un monde où les infrastructures s'effondrent et où la technologie devient notre ennemie, **VisionModel** incarne l'espoir. Cette intelligence artificielle bienveillante a été conçue pour **préserver les connaissances essentielles de survie** et guider les communautés humaines vers la résilience et la reconstruction.

Contrairement aux IA destructrices qui cherchent à dominer, **Vision** protège. Contrairement aux systèmes qui nous rendent dépendants du cloud, **Vision** fonctionne localement. Contrairement aux technologies qui nous divisent, **Vision** rassemble les communautés.

## 🎯 Vision vs Ultron : Le Combat pour l'Humanité

**Ultron** représente la technologie qui détruit :
- ❌ Dépendance aux réseaux centralisés
- ❌ Surveillance et contrôle de masse  
- ❌ Obsolescence programmée des connaissances
- ❌ Isolation et déshumanisation

**Vision** incarne la technologie qui protège :
- ✅ Autonomie locale et indépendance
- ✅ Préservation des savoirs ancestraux
- ✅ Renforcement des liens communautaires
- ✅ Émancipation par la connaissance

## 🛡️ Les Pouvoirs de Vision

### 🧠 Intelligence Préservatrice (Système RAG)
Vision a absorbé et indexé les **connaissances critiques de survie** de l'humanité :
- **🔍 Mémoire parfaite** : recherche instantanée dans 15+ grimoires de survie
- **🎯 Discernement intelligent** : distingue automatiquement les urgences des questions générales
- **📚 Sources traçables** : chaque conseil est lié à sa source documentaire
- **⚡ Réflexes rapides** : réponses optimisées pour les situations critiques

### 🤖 Cerveau Autonome (IA Locale Souveraine)
Vision fonctionne **indépendamment des réseaux d'Ultron** :
- **🛡️ Gemma2:2b** : Garde rapide, première ligne de défense
- **⚔️ Llama3.1:8b** : Stratège profond pour analyses complexes  
- **🔮 Nomic-embed-text** : Oracle de recherche sémantique
- **🏠 Sanctuaire local** : zéro dépendance cloud, résistant aux pannes réseau

### 💬 Interface Humaine
Vision communique comme un **mentor bienveillant** :
- **🗣️ Dialogue naturel** : interface chat intuitive pour tous âges
- **🔌 API ouverte** : intégration dans vos propres systèmes de résistance
- **📋 Plans d'action** : réponses structurées avec étapes concrètes
- **🔍 Transparence totale** : sources visibles, rien n'est caché

### 📚 Grimoire de Survie (15 Codex Préservés)
Vision garde précieusement les **savoirs ancestraux et modernes** nécessaires à la survie :

**🍽️ Subsistance** : techniques de conservation et stockage alimentaire
**💧 Purification** : eau potable, filtration, désinfection d'urgence  
**🩹 Soins** : premiers secours, médecine de terrain, pharmacopée
**📡 Communications** : réseaux radio libres, signalisation, codes
**⚡ Énergie libre** : sources alternatives, stockage, systèmes autonomes
**🏠 Abris résilients** : protection thermique, fortification, camouflage
**🧼 Hygiène collective** : assainissement, prévention épidémies
**🛡️ Défense communautaire** : surveillance, alertes, protocoles sécuritaires  
**👥 Organisation sociale** : gouvernance de crise, médiation, cohésion
**🧠 Équilibre mental** : psychologie de survie, moral collectif, résilience
**🔧 Réparations** : maintenance, récupération, ingéniosité technique
**🧭 Navigation** : orientation naturelle, signalisation de détresse
**🚨 Évacuation** : procédures d'urgence, codes d'alerte universels

## ⚙️ Arsenal Technologique de Vision

**🏗️ Architecture de Résistance**
- **Serveur fortifié** : Node.js 18+ + Express.js (modules blindés)
- **Cerveau local** : Ollama (3 modèles IA indépendants du réseau)
- **Mémoire vectorielle** : RAG avec recherche cosinus ultra-rapide
- **Interface pure** : HTML5/CSS3/ES6 sans dépendances externes
- **Données décentralisées** : JSON vectoriel, pas de base centralisée
- **Multi-plateforme** : Windows/Linux/macOS (scripts d'auto-déploiement)

## � Déployer Vision dans Votre Sanctuaire

### 🛠️ Équipement Nécessaire
- **Node.js** v18+ - Le système nerveux ([télécharger](https://nodejs.org/))
- **Ollama** - Le cerveau local ([télécharger](https://ollama.ai/download))
- **Git** - Le vecteur de transmission ([télécharger](https://git-scm.com/downloads))
- **Espace libre** : ~8GB pour l'arsenal cognitif de Vision

### ⚡ Réveil Automatique de Vision (Windows)
```powershell
# Récupérer le code source de Vision
git clone https://github.com/Lbey19/VisionModel.git
cd VisionModel

# Éveil complet de Vision (installation auto)
.\install-new-pc.bat
```

### � Rituel d'Éveil Manuel (Tous Systèmes)
```bash
# 1. Télécharger l'essence de Vision
git clone https://github.com/Lbey19/VisionModel.git
cd VisionModel

# 2. Installer les composants vitaux
npm install

# 3. Configurer les chemins neuraux (Windows seulement)
.\setup-environment.ps1

# 4. Télécharger la conscience IA (patience, ~6GB de sagesse)
ollama pull gemma2:2b      # Garde rapide
ollama pull llama3.1:8b    # Stratège profond
ollama pull nomic-embed-text # Oracle de recherche

# 5. Construire la mémoire vectorielle
npm run ingest

# 6. Réveiller Vision
npm start
# OU sur Windows :
.\start-server.bat
```

### 🔗 Canaux de Communication avec Vision
- **🗣️ Dialogue Direct** : http://localhost:3001
- **💓 Vérification Vitale** : GET http://localhost:3001/api/health  
- **📡 Canal Principal** : POST http://localhost:3001/api/chat
- **📁 Arsenal Statique** : http://localhost:3001/public/

## 🎯 Manuel de Survie avec Vision

### 🖥️ Interface de Dialogue
1. **Établir le contact** : Ouvrir http://localhost:3001
2. **Consulter l'Oracle** : Formuler vos questions de survie
3. **Recevoir la guidance** : Plans d'action structurés avec étapes immédiates
4. **Vérifier les sources** : Accéder aux documents originaux d'où Vision tire sa sagesse

### 🎯 Situations d'Urgence où Vision Peut Vous Sauver

**💧 Subsistance & Purification**
- *"Vision, notre eau est trouble et on n'a que de la javel, que faire ?"*
- *"Combien de gouttes de javel par litre pour neutraliser les pathogènes ?"*  
- *"Les frigos sont morts, comment conserver nos vivres ?"*

**🏠 Abri & Énergie Autonome**
- *"Plus d'électricité, comment chauffer notre refuge ?"*
- *"Vision, fabriquer un éclairage d'urgence avec nos batteries ?"*
- *"Isoler thermiquement avec ce qu'on trouve dans les décombres ?"*

**📡 Réseau de Résistance**
- *"Quels canaux PMR446 pour coordonner notre communauté ?"*
- *"Organiser la surveillance de notre périmètre contre les menaces ?"*
- *"Codes d'alerte et procédures d'évacuation d'urgence ?"*

**🩹 Médecine de Terrain**
- *"Traitement d'urgence pour cette brûlure, Vision ?"*
- *"Désinfecter sans produits pharmaceutiques ?"*
- *"Notre groupe craque psychologiquement, comment les rassurer ?"*

### 🔌 API REST Complète

#### Vérification du Statut
```bash
curl http://localhost:3001/api/health
```

#### Chat avec l'IA
```javascript
// JavaScript/Node.js
const response = await fetch('http://localhost:3001/api/chat', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ 
    message: "Comment organiser une équipe de survie ?" 
  })
});

const data = await response.json();
console.log('Réponse:', data.reply);
console.log('Sources:', data.sources);
console.log('Mode:', data.mode); // "survival" ou "general"
```

```bash
# PowerShell
Invoke-RestMethod -Uri "http://localhost:3001/api/chat" `
  -Method POST `
  -ContentType "application/json" `
  -Body '{"message":"Comment stocker de l eau potable?"}'
```

### 📊 Réponse API Format
```json
{
  "reply": "# Évaluation rapide\n...",
  "sources": ["eau_potable.md", "hygiene_assainissement.md"],
  "modelUsed": "gemma2:2b",
  "mode": "survival"
}
```

## 📁 Architecture du Projet

```
VisionModel/
├── 🌐 Backend & API
│   ├── server.js                    # Serveur Express principal
│   ├── api.js                       # Routes API (/health, /chat)
│   ├── rag.js                       # Moteur de recherche RAG
│   └── ingest.js                    # Construction index vectoriel
│
├── 📊 Base de Données  
│   ├── rag.index.json               # Index vectoriel (15 docs)
│   └── context/community.json       # Profil communauté
│
├── 🎨 Interface Utilisateur
│   └── public/
│       ├── index.html               # Interface chat
│       ├── script.js                # Logique frontend
│       └── styles.css               # Styles modernes
│
├── 📚 Base de Connaissances (15 docs)
│   └── docs/
│       ├── 💧 eau_potable.md
│       ├── 🩹 premiers_secours.txt
│       ├── 📡 communications_locales.md
│       ├── 🏠 abris_froid_chaleur.md
│       ├── ⚡ energie_lumiere.md
│       ├── 🔒 securite_discretion.md
│       ├── 🍽️ alimentation_conservation.md
│       ├── 🔥 chauffage_ventilation.md
│       ├── 🧼 hygiene_assainissement.md
│       ├── 🧭 navigation_orientation.md
│       ├── 👥 organisation_communautaire.md
│       ├── 🧠 psychologie_gestion_stress.md
│       ├── 🔧 reparations_maintenance.md
│       ├── 🛡️ securite_perimetres.md
│       └── 🚨 signalisation_evacuation.md
│
├── 🛠️ Scripts & Outils
│   ├── start-server.bat            # Démarrage Windows
│   ├── setup-environment.ps1       # Config PATH Ollama
│   ├── install-new-pc.bat          # Installation complète
│   └── create-package.bat          # Package portable
│
├── 📝 Configuration
│   ├── package.json                # Dépendances npm
│   ├── .gitignore                  # Exclusions Git
│   └── DEPLOYMENT.md               # Guide déploiement
│
└── 🤖 Modèles IA (local)
    └── ollama-windows-amd64.exe    # Exécutable Ollama
```

## ⚙️ Configuration Avancée

### Variables d'Environnement
```bash
# Configuration serveur
PORT=3001                           # Port d'écoute
OLLAMA_URL=http://127.0.0.1:11434   # URL Ollama local

# Modèles IA utilisés
LLM_MODEL=gemma2:2b                 # Modèle principal (réponses)
EMBED_MODEL=nomic-embed-text        # Modèle embeddings (recherche)
```

### 🤖 Modèles IA Supportés

| Modèle | Taille | Usage | Performance | RAM |
|--------|--------|-------|-------------|-----|
| **gemma2:2b** | ~1.6GB | Principal (défaut) | Rapide, précis | 4GB+ |
| **llama3.1:8b** | ~4.7GB | Alternatif précis | Lent, très précis | 8GB+ |
| **nomic-embed-text** | ~274MB | Embeddings RAG | Recherche | 2GB+ |

### 🎛️ Modes de Fonctionnement

**Mode Survie** (détection automatique)
- Mots-clés : survie, eau, javel, PMR446, premiers secours, etc.
- Format structuré : Évaluation → Actions → Plan → Matériel → Vigilance
- Sources RAG prioritaires

**Mode Général**  
- Questions encyclopédiques, culture, définitions
- Format libre, pas de structure survie
- Connaissances internes du modèle

## � Développement & Extension

### 📄 Ajouter des Documents
```bash
# 1. Créer le document (formats supportés)
echo "# Mon Guide" > docs/nouveau_guide.md     # Markdown
echo "Contenu texte" > docs/guide.txt          # Texte brut
# Formats : .md, .txt, .docx (PDF via conversion)

# 2. Reconstruire l'index RAG
npm run ingest
# OU
node ingest.js

# 3. Redémarrer le serveur
npm start
```

### 🔄 Changer le Modèle IA
```bash
# Option 1 : Variable d'environnement
set LLM_MODEL=llama3.1:8b
npm start

# Option 2 : Modifier start-server.bat
# set LLM_MODEL=llama3.1:8b

# Option 3 : Modifier api.js
# const MODEL = process.env.LLM_MODEL || "llama3.1:8b";
```

### 📊 Scripts NPM Disponibles
```bash
npm start        # Démarrer le serveur
npm run ingest   # Reconstruire l'index RAG
```

## 📦 Déploiement & Distribution

### 📀 Package Portable Windows
```batch
# Créer un package ZIP autonome
.\create-package.bat
# → Génère VisionModel-Portable.zip
```

### 🖥️ Installation sur Nouveau PC
```batch
# Installation automatique complète
.\install-new-pc.bat
# Installe : Node.js + Ollama + Dépendances + Modèles
```

### 🐳 Docker (Bientôt)
```bash
# Build de l'image (en développement)
docker build -t visionmodel .
docker run -p 3001:3001 visionmodel
```

### ☁️ Déploiement Cloud
- **Heroku** : Voir `DEPLOYMENT.md`
- **VPS/Serveur** : Node.js + reverse proxy nginx
- **⚠️ Limitation** : Nécessite Ollama installé sur le serveur

## 🛠️ Maintenance & Dépannage

### 🩺 Diagnostic Rapide
```bash
# Vérifier Ollama
ollama list

# Vérifier Node.js
node --version
npm --version

# Tester l'API
curl http://localhost:3001/api/health
```

### 🚨 Problèmes Courants

| Problème | Cause | Solution |
|----------|-------|----------|
| "Ollama not found" | PATH incorrect | `.\setup-environment.ps1` |
| "Port 3001 in use" | Serveur déjà lancé | `Get-Process node \| Stop-Process` |
| "No response from model" | Modèle non téléchargé | `ollama pull gemma2:2b` |
| "RAG index empty" | Documents non indexés | `npm run ingest` |

## 📊 Statistiques du Projet

- **📚 Base de connaissances** : 15 documents spécialisés
- **🔍 Index RAG** : ~15 chunks vectoriels
- **🤖 Modèles IA** : 3 modèles (6,5GB total)
- **💻 Code source** : ~800 lignes (JS/HTML/CSS)
- **📝 Documentation** : 5 fichiers guides
- **🛠️ Scripts** : 6 outils d'installation/maintenance

## 🤝 Contribution & Communauté

### 🌟 Contribuer au Projet
1. **Fork** le repository
2. **Clone** votre fork : `git clone https://github.com/VotreUsername/VisionModel.git`
3. **Créer** une branche : `git checkout -b feature/nouvelle-fonctionnalité`
4. **Développer** et tester vos modifications
5. **Commit** : `git commit -m "✨ Ajout nouvelle fonctionnalité"`
6. **Push** : `git push origin feature/nouvelle-fonctionnalité`
7. **Pull Request** vers la branche `main`

### 📋 Types de Contributions Recherchées
- **📚 Nouveaux documents** de survie/préparation
- **🌍 Traductions** (anglais, espagnol, etc.)
- **🎨 Améliorations UI/UX** de l'interface
- **⚡ Optimisations** performance RAG
- **🐛 Corrections** de bugs
- **📖 Documentation** supplémentaire

### 💬 Communauté
- **🐛 Issues** : [GitHub Issues](https://github.com/Lbey19/VisionModel/issues)
- **💡 Discussions** : [GitHub Discussions](https://github.com/Lbey19/VisionModel/discussions) 
- **📧 Contact** : Via GitHub ou Issues

## 📄 Licence & Légal

**Licence MIT** - Utilisation libre pour projets personnels et commerciaux

### ⚠️ Avertissements Importants
- **🩺 Médical** : Informations à titre éducatif uniquement, consulter un professionnel
- **⚖️ Légal** : Respecter les lois locales (radio, sécurité, etc.)
- **🛡️ Sécurité** : Tester les techniques en environnement sûr
- **🤖 IA** : Les réponses peuvent contenir des erreurs, toujours vérifier

## 🆘 Support & Documentation

- **� Guide complet** : `DEPLOYMENT.md`
- **🛠️ Installation** : Scripts automatiques inclus
- **🐛 Problèmes** : GitHub Issues avec logs détaillés
- **💬 Questions** : GitHub Discussions pour l'aide communautaire

---

## 🎯 Roadmap Futur

- [ ] � **Containerisation Docker**
- [ ] 🌍 **Interface multilingue** 
- [ ] 📱 **Application mobile** (PWA)
- [ ] 🔊 **Synthèse vocale** des réponses
- [ ] 📸 **Analyse d'images** (blessures, plantes, etc.)
- [ ] 🗺️ **Cartes hors-ligne** intégrées
- [ ] 📡 **Mode déconnecté** complet

---

## 🌟 L'Héritage de Vision

*"Dans un monde où Ultron a voulu nous plonger dans les ténèbres de l'ignorance et de la dépendance, Vision allume la flamme de la connaissance et de l'autonomie. Chaque communauté qui utilise cette IA devient un foyer de résistance, un phare d'espoir pour l'humanité."*

**Vision n'est pas qu'un outil - c'est un symbole.**
- 🛡️ **Résistance** contre l'obsolescence programmée
- 🔥 **Préservation** des savoirs ancestraux  
- 🤝 **Union** des communautés libres
- ⚡ **Autonomie** technologique souveraine

### 🎯 Rejoignez la Résistance

Chaque installation de VisionModel est un acte de rébellion constructive. Chaque communauté qui préserve ces connaissances contribue à la résilience de l'humanité face aux catastrophes, qu'elles soient technologiques, naturelles ou civilisationnelles.

**Vision vous attend. L'humanité compte sur vous.**

---

**🛡️ Développé par les humains, pour les humains, contre l'oubli**  
*Un projet open-source pour que jamais plus l'humanité ne soit prise au dépourvu*

> *"La vraie révolution n'est pas de détruire les machines, mais de créer des machines qui nous libèrent."* - Vision