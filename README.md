# ğŸ¤– VisionModel - IA RAG System

## ğŸ“ Description
SystÃ¨me d'IA conversationnelle basÃ© sur RAG (Retrieval-Augmented Generation) utilisant Ollama pour traiter des documents de survie et fournir des rÃ©ponses contextuelles prÃ©cises.

## âœ¨ FonctionnalitÃ©s
- ğŸ” **RAG System** : Recherche sÃ©mantique dans les documents
- ğŸ¤– **IA Locale** : Utilise Ollama avec Gemma3:1b et Llama3.1:8b  
- ğŸ’¬ **Interface Web** : Chat interactif simple et efficace
- ğŸ“š **Base de connaissances** : Documents de survie spÃ©cialisÃ©s
- âš¡ **RÃ©ponses rapides** : OptimisÃ© pour des rÃ©ponses sous 60 secondes

## ğŸ› ï¸ Technologies
- **Backend** : Node.js + Express
- **IA** : Ollama (Gemma3:1b, Llama3.1:8b, nomic-embed-text)
- **RAG** : Recherche vectorielle sÃ©mantique
- **Frontend** : HTML/CSS/JS vanilla
- **OS** : Compatible Windows/Linux/macOS

## ğŸš€ Installation

### PrÃ©requis
- [Node.js](https://nodejs.org/) (v18+)
- [Ollama](https://ollama.ai/download)

### Installation rapide
```bash
# Cloner le projet
git clone https://github.com/Lbey19/VisionModel.git
cd VisionModel

# Installer les dÃ©pendances
npm install

# Installer les modÃ¨les IA
ollama pull gemma3:1b
ollama pull llama3.1:8b  
ollama pull nomic-embed-text

# DÃ©marrer le serveur
npm start
# ou
.\start-server.bat  # Windows
```

### AccÃ¨s
- **Interface Web** : http://localhost:3001
- **API Health** : http://localhost:3001/health
- **API Chat** : POST http://localhost:3001/ai

## ğŸ“– Utilisation

### Interface Web
1. Ouvrir http://localhost:3001
2. Poser des questions sur la survie
3. Recevoir des rÃ©ponses basÃ©es sur les documents

### Exemples de questions
- "Combien de litres d'eau par personne ?"
- "Comment purifier l'eau ?"
- "Quels canaux PMR446 utiliser ?"
- "Comment traiter une blessure ?"

### API
```javascript
// Exemple d'appel API
fetch('http://localhost:3001/ai', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({ message: "Comment stocker l'eau ?" })
})
.then(r => r.json())
.then(data => console.log(data.reply));
```

## ğŸ“ Structure du projet
```
VisionModel/
â”œâ”€â”€ server.js              # Serveur principal
â”œâ”€â”€ package.json           # DÃ©pendances npm
â”œâ”€â”€ rag.js                 # Module RAG 
â”œâ”€â”€ rag.index.json         # Index vectoriel
â”œâ”€â”€ start-server.bat       # Script de dÃ©marrage Windows
â”œâ”€â”€ public/                # Interface web
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ style.css
â”‚   â””â”€â”€ script.js
â”œâ”€â”€ docs/                  # Documents de base de connaissances
â”‚   â”œâ”€â”€ eau_potable.md
â”‚   â”œâ”€â”€ premiers_secours.txt
â”‚   â”œâ”€â”€ communications_locales.md
â”‚   â”œâ”€â”€ abris_froid_chaleur.md
â”‚   â”œâ”€â”€ energie_lumiere.md
â”‚   â””â”€â”€ securite_discretion.md
â””â”€â”€ DEPLOYMENT.md          # Guide de dÃ©ploiement
```

## ğŸ”§ Configuration

### Variables d'environnement
```env
PORT=3001
OLLAMA_URL=http://127.0.0.1:11434
MODEL=gemma3:1b
```

### ModÃ¨les IA disponibles
- **gemma3:1b** : Rapide, optimisÃ© (par dÃ©faut)
- **llama3.1:8b** : Plus prÃ©cis, plus lent
- **nomic-embed-text** : Embeddings pour la recherche

## ğŸ› ï¸ DÃ©veloppement

### Ajouter des documents
1. Placer les fichiers dans `/docs/`
2. Relancer le serveur pour rÃ©indexation automatique

### Modifier les modÃ¨les
```javascript
// Dans server.js
const MODEL = "llama3.1:8b"; // Changer ici
```

## ğŸ“¦ DÃ©ploiement

### Package portable
```bash
.\create-package.bat  # CrÃ©e VisionModel-Portable.zip
```

### Sur un nouveau PC
1. Copier les fichiers
2. Installer Node.js et Ollama  
3. ExÃ©cuter `.\install-new-pc.bat`

## ğŸ¤ Contribution
1. Fork le projet
2. CrÃ©er une branche feature
3. Commit vos changes
4. Push et crÃ©er une Pull Request

## ğŸ“„ Licence
MIT License - Voir le fichier LICENSE

## ğŸ†˜ Support
- ğŸ“§ Issues GitHub pour les bugs
- ğŸ’¬ Discussions pour les questions
- ğŸ“– Consulter DEPLOYMENT.md pour l'installation

---
DÃ©veloppÃ© avec â¤ï¸ pour la communautÃ© survivaliste